{
  "hash": "937157f9eb913f088afc2de9410b561f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nknitr: true\nsyntax-definition: \"../Assets/zig.xml\"\n---\n\n\n\n\n\n\n\n\n# Project 4 - Developing an image filter\n\nIn this chapter we are going to build a new small project. The objective of\nthis project is to build a program in Zig that applies a filter over an image.\nMore specifically, a \"grayscale filter\". This filter essentially transforms\nany color image into a grayscale image.\n\nWe are going to use the image displayed at @fig-pascal as the example in this project.\nIn other words, we are going to transform this colored image, into a grayscale image,\nusing our \"image filter program\" written in Zig.\n\n![A photo of the chilean-american actor Pedro Pascal. Source: Google Images.](../ZigExamples/image_filter/pedro_pascal.png){#fig-pascal}\n\nWe don't need to write a lot of code to build such \"image filter program\". However, in order for us\nto build such program, we first need to understand how digital images work. That is why we begin this chapter\nby explaining the theory behind digital images and how colors are represented in modern computers.\nWe also give a brief explanation about the file format PNG (Portable Network Graphics), which is used\nin the example images.\n\nAt the end of this chapter, we will have a full example of a program that takes the PNG image displayed at @fig-pascal\nas input, and writes a new image to the current working directory that is the grayscale version of the input image.\nThis grayscale version of @fig-pascal that is written by the program is exposed at @fig-pascal-gray.\n\n![The grayscale version of the photo.](../ZigExamples/image_filter/pedro_pascal_filter.png){#fig-pascal-gray}\n\n\n## How we see things? {#sec-eyes}\n\nIn this section, I want to briefly describe to you how we (humans) actually see things with our own eyes.\nI mean, how our eyes work? If you do have a very basic understanding of how our eyes work, you will understand\nmore easily how digital images are made. Because the techniques used to produce a digital image\nwere developed by taking a lot of inspiration from how our human eyes work.\n\nYou can interpret a human eye as a light sensor, or, a light receptor. The eye receives some amount of light as input,\nand it interprets the colors that are present in this \"amount of light\".\nIf no amount of light hits the eye, then, the eye cannot extract color from it, and as result,\nwe end up seeing nothing, or, more precisely, we see complete blackness.\n\nSo everything depends on light. What we actually see are the colors (blue, red, orange, green, purple, yellow, etc.) that\nare being reflected from the light that is hitting our eyes. **Light is the source of all colors!**\nThis is what Isaac Newton discovered on his famous prism experiment[^newton] in the 1660s.\n\n[^newton]: <https://library.si.edu/exhibition/color-in-a-new-light/science>\n\nInside our eyes, we have a specific type of cell called the \"cone cell\".\nOur eye have three different types, or, three different versions of these \"cone cells\".\nEach of these three types of cone cell is very sensitive to a specific spectrum of the light,\nwhich are the spectrums that define the colors red, green and blue.\nSo, in summary, our eyes have specific types of cells that\nare highly sensitive to these three colors (red, green and blue).\n\nThese are the cells responsible for perceiving the color present in the light that hits our eyes.\nAs a result, our eyes perceives color as a mixture of these three colors (red, green and blue). By having an amount\nof each one of these three colors, and mixing them together, we can get any other visible color\nthat we want. So every color that we see is perceived as a specific mixture of blues, greens and reds,\nlike 30% of red, plus 20% of green, plus 50% of blue.\n\nWhen these cone cells perceive (or, detect) the colors that are found in the\nlight that is hitting our eyes, these cells produce electrical signals and sent them to the brain.\nOur brain interprets these electrical signals, and use them to form the image that we are seeing\ninside our head.\n\nBased on what we have discussed here, the items below describes the sequence of events that\ncomposes this very simplified version of how our human eyes work:\n\n1. Light hits our eyes.\n1. The cone cells perceive the colors that are present in this light.\n1. Cone cells produce electrical signals that describes the colors that were perceived in the light.\n1. The electrical signals are sent to the brain.\n1. Brain interprets these signals, and form the image based on the colors identified by these electrical signals.\n\n\n## How digital images work? {#sec-digital-img}\n\nA digital image is a \"digital representation\" of an image that we see with our eyes.\nIn other words, a digital image is a \"digital representation\" of the colors that we see\nand perceive through the light.\nIn the digital world, we have two types of images, which are: vector image and raster image.\nVector images are not described here. So just remember that every single aspect that we discuss\nhere in this chapter about digital images **are related solely to raster images**, and not vector images.\n\nRaster images are digital images that are represented as a 2D (two dimensional) matrix\nof pixels. In other words, every raster image is basically a rectangle of pixels. Each pixel have a particular color.\nSo, a raster image is just a rectangle of pixels, and each of these pixels are displayed in the screen of your computer (or the screen\nof any other device, e.g. laptop, tablet, smartphone, etc.) as a color.\n\n@fig-raster demonstrates this idea. If you take any raster image, and you zoom into it very hard,\nyou will see the actual pixels of the image. Although JPEG and PNG are file formats to store raster images,\nwhen you zoom into JPEG, PNG, and some other types of raster image files, you usually do not quite see the pixels.\nThat is because most of these file formats implement techniques that affect how the pixels are displayed,\nwith the objective of increasing the details in the image. The most famous of these techniques is anti-aliasing,\nwhich you probably know from video-games. But nevertheless, the pixels are still there! They are\njust displayed differently in JPEG/PNG files.\n\n![Zooming over a raster image to see the pixels. Source: Google Images.](../Figures/imagem-raster.png.webp){#fig-raster}\n\nThe more pixels the image has, the more information and detail we can include in the image.\nThe more precise, sharp and pretty will look the image. That is why photographic cameras\nusually produces big raster images, with several megapixels of resolution, to include as much detail as possible into the final image.\nAs an example, a digital image with dimensions of 1920 pixels of width and 1080 pixels of height, would be a image that\ncontains $1920 \\times 1080 = 2073600$ pixels in total. You could also say that the \"total area\" of the image is\nof 2073600 pixels, although the concept of \"area\" is not very used here in computer graphics.\n\nMost digital images we see in our modern world uses the RGB color model. RGB stands for (red, green and blue) if you did not\ndetected that yet. So the color of each pixel in these raster images are usually\nrepresented as a mixture of red, green and blue, just like in our eyes. That is, the color of each pixel is identified by a set of\nthree different integer values. Each integer value identifies the \"amount\" of each color (red, green and blue).\nFor example, the set `(199, 78, 70)` identifies a color that is close to red. We have 199 of red, 78 of green,\nand 70 of blue. In contrast, the set `(129, 77, 250)` describes a color that is more close to purple. Et cetera.\n\n\n\n### Images are displayed from top to bottom\n\nThis is not a rule written in stone, but the big majority of digital images are displayed from top\nto bottom and left to right. Most computers screens also follow this pattern. So, the first pixels\nin the image are the ones that are at the top and left corner of the image. You can find a visual representation\nof this logic at @fig-img-display.\n\nAlso notice in @fig-img-display that, since a digital image is essentially a 2D matrix of pixels,\nthe image is organized into rows and columns of pixels. The columns are defined by the horizontal x axis,\nwhile the rows are defined by the vertical y axis.\n\nEach pixel (i.e. the gray rectangles) exposed at @fig-img-display contains a number inside of it.\nThese numbers are the indexes of the pixels. You can notice that the first pixels are in the top and left\ncorner, and also, that the indexes of these pixels \"grow to the sides\", or, in other words, they grow in the direction of the horizontal x axis.\nThis means that most digital images are usually organized as rows of pixels. So when these digital images are\ndisplayed, the screen display the first row of pixels, then, the second row, then, the third row, etc.\n\n![How the pixels of raster images are displayed.](./../Figures/image-display.png){#fig-img-display}\n\n\n\n\n\n\n### Representing the matrix of pixels in code {#sec-pixel-repr}\n\nOk, we know already that digital images are represented as a 2D matrix of pixels.\nBut we do not have a notion of a 2D matrix in Zig. Most low-level languages in general (Zig, C, Rust, etc.) do not have such notion.\nSo how do we represent such matrix of pixels in Zig, or any other low-level language.\nThe strategy that most programmers choose is to just use a normal 1D array to store the values of\nthis 2D matrix. In other words, you just create an normal 1D array, and store all values from both dimensions into this 1D array.\n\n\nNow, remember, a digital image is represented as a\n2D matrix of pixels, and each pixel is represented by 3 unsigned 8bit-integer values.\nSo, if we have for example a very small image of dimensions 4x3, then,\nwe have 12 pixels in total in this image. As a result, we need to create a normal array that\ncan store $3 \\times 12 = 36$ integer values, more precisely, an array of 36 `u8` values.\n\nThe reason why unsigned 8-bit integer (`u8`) values are used to represent the amounts of each color,\ninstead of any other integer type, is because they take the minimum amount of space as possible, or,\nthe minimum amount of bits as possible. Which helps to reduces the binary size of the image, i.e. of the 2D matrix.\nAlso, they convey a good amount of precision and detail about the colors, even though they can represent\na relatively small range (from 0 to 255) of \"color amounts\".\nThe `matrix` object exposed below could be an example of this 4x3 image.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst matrix = [_]u8{\n    201, 10, 25, 185, 65, 70,\n    65, 120, 110, 65, 120, 117,\n    98, 95, 12, 213, 26, 88,\n    143, 112, 65, 97, 99, 205,\n    234, 105, 56, 43, 44, 216,\n    45, 59, 243, 211, 209, 54,\n};\n```\n:::\n\n\n\n\n\nThe three first integer values are the color amounts of the first pixel.\nWhile the next three integer are the colors amounts for the second pixel.\nAnd the sequence goes on in this pattern. So the size of the array that stores\nthe values of the pixels from a raster image is a multiple of 3.\nIn this case, the array have size of 36.\n\nThe size of the array can also be a multiple of 4 if a transparency amount is\nalso included into the raster image. In other words, there are some raster images\nthat follow a different color model, which is the RGBA (red, green, blue and alpha)\ncolor model. The \"alpha\" corresponds to an amount of transparency in the pixel.\nSo every pixel in a RGBA image is represented by a red, green, blue and alpha values.\n\nMost raster images uses the standard RGB model, so, for the most part, you will\nsee arrays sizes that are multiples of 3. But some images, specially the ones\nthat are stored in PNG files, might be using the RGBA model, and, therefore, are\nrepresented by an array whose size is a multiple of 4.\n\nIn our case here, the example image of our project (@fig-pascal) is a raster image\nstored in a PNG file, and this specific image is using the RGBA color model. So\neach pixel in the image is represented by 4 different integer values, and, as consequence,\nto store this image in our Zig code, we need to create an array whose size is a multiple of 4.\n\n\n## The PNG library we are going to use\n\nLet's begin our project by focusing on writing the necessary Zig code to\nread the data from the PNG file. In other words, in this section, we want\nto read the PNG file exposed at @fig-pascal, and parse it's data to extract the 2D matrix of pixels\nthat represents the image.\n\nAs we have discussed at @sec-pixel-repr, the image that we are using as example here\nis a PNG file that uses the RGBA color model, and, threfore, each pixel of the image\nis represented by 4 integer values. You can download this image by visiting the `ZigExamples/image_filter`\n[folder at the official repository of this book](https://github.com/pedropark99/zig-book/tree/main/ZigExamples/image_filter)[^img-filter-folder].\nYou can also find in this folder the complete source code of this small project that we\nare developing here.\n\n[^img-filter-folder]: <https://github.com/pedropark99/zig-book/tree/main/ZigExamples/image_filter>\n\nThere are some C libraries available that we can use to read and parse PNG files.\nThe most famous and used of all is the `libpng`, which is the \"official library\" for reading and writing\nPNG files. This C library is available in most operating system. But this C library is very known\nfor being a little complex and hard to use.\n\nThat is why, I'm going to use a more modern alternative here in this project, which is the `libspng` library.\nI have choose to use this C library here, because it is much, much\nsimpler to use than `libpng`, and also, offers very good performance for all operations.\nYou can checkout the [official website of the library](https://libspng.org/)[^libspng]\nto know more about it.\nYou will also find there, some documentation that might help you to understand and\nfollow the code examples exposed here.\n\n[^libspng]: <https://libspng.org/>\n\n\nFirst of all, remember to build and install this `libspng` into your system. Because\nif don't do this step first, then, the `zig` compiler cannot find the files and resources of\nthis library in your computer, and link them with the Zig code that we are going to write here\ntogether. There is good information about how to build and install the library at the\n[build section of the library documentation at the official website](https://libspng.org/docs/build/)[^lib-build].\n\n[^lib-build]: <https://libspng.org/docs/build/>\n\n\n## How to call C code from Zig\n\nWe have discussed at @sec-building-c-code how to build C code using the `zig` compiler.\nBut we haven't discussed yet how to actually use C code in Zig. In other words,\nwe haven't discussed yet how to call C code from Zig, or, how to use C libraries in your Zig code,\netc.\n\nLuckily enough, the small project that we are going to develop in this chapter will use and\ncall some C code. So now is the opportunity that we have to demonstrate this aspect of Zig.\nSo yes, you can use C code directly in Zig. Because of the very good interoperability between\nZig and C, it is possible to use and call code from any C library that you want in Zig.\n\nThis is not something new. Most high-level programming languages have FFI (foreign function interfaces),\nwhich can be used to call C code. For example, Python have Cython, R have `.Call()`, Javascript have `ccall()`, etc.\nBut Zig integrates with C in a more direct way. You can call C functions directly in your Zig code without the need\nto use a FFI.\n\nEverything you have to do, is to import into your Zig code the C header file that describes the C functions\nthat you are going to use, pretty much like you would do in C, by including the header files into your C module.\nAfter you import the header file, you can start calling and using the C functions described in this C header file\nin your Zig code.\nBut remember, whenever you use a C library in your Zig code, you have to link your Zig code with this\nC library in your build process. To do that, you use the techniques that we have already\ndiscussed across the @sec-build-system.\n\n\n### Importing C header files\n\nTo import C header files into our Zig code, we use the built-in functions `@cInclude()` and `@cImport()`.\nInside the `@cImport()` function, we open a block (with a pair of curly braces). Inside this block\nwe can include `@cDefine()` calls to define C macros when including this specific C header.\nBut we can also include a single `@cInclude()` call inside `@cImport()`.\n\nYou provide the name of the C header that you want to include as input to `@cInclude()`.\nTake the example below, where we are importing the Standard IO C Library (`stdio.h`),\nand calling the `printf()` C function. Notice that we have also used the C function `powf()`,\nwhich comes from the C Math Library (`math.h`).\nIn order to compile this example, you have to link this Zig code with both\nthe C Standard Library and the C Math Library, by passing the flags `-lc` and `-lm`\nto the `zig` compiler.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst cmath = @cImport({\n    @cInclude(\"math.h\");\n});\nconst stdio = @cImport({\n    @cDefine(\"_NO_CRT_STDIO_INLINE\", \"1\");\n    @cInclude(\"stdio.h\");\n});\n\npub fn main() !void {\n    const x: f32 = 15.2;\n    const y = cmath.powf(x, @as(f32, 2.6));\n    _ = stdio.printf(\"%.3f\\n\", y);\n}\n```\n:::\n\n\n\n\n\n```\n1182.478\n```\n\n\n<!--\n### About passing inputs to C functions\n\nZig objects have some intrinsic differences between their C equivalents.\nProbably the most noticeable difference is the difference between C strings and Zig strings.\nZig strings are objects that contains an arrays of arbitrary bytes and a length value.\nOn the other hand, a C string is a null terminated array of arbitrary bytes.\n\nIn some cases, the `zig` compiler can complain about the use of Zig objects\nbeing given as input to the C function.\n-->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}