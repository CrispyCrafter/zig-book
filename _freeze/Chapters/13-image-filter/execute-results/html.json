{
  "hash": "5c4be2b858dd63729fed93c19610e17c",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nknitr: true\nsyntax-definition: \"../Assets/zig.xml\"\n---\n\n\n\n\n\n\n\n# Project 4 - Developing an image filter\n\nIn this chapter we are going to build a new small project. The objective of\nthis project is to build a program in Zig that applies a filter over an image.\nMore specifically, a \"grayscale filter\". This filter essentially transforms\nany color image into a grayscale image.\n\nWe are going to use the image displayed at @fig-pascal as the example in this project.\nIn other words, we are going to transform this colored image, into a grayscale image,\nusing our \"image filter program\" written in Zig.\n\n![A photo of the chilean-american actor Pedro Pascal. Source: Google Images.](../ZigExamples/image_filter/pedro_pascal.png){#fig-pascal}\n\nWe don't need to write a lot of code to build such \"image filter program\". However, in order for us\nto build such program, we first need to understand how digital images work. That is why we begin this chapter\nby explaining the theory behind digital images and how colors are represented in modern computers.\nWe also give a brief explanation about the file format PNG (Portable Network Graphics), which is used\nin the example images.\n\nAt the end of this chapter, we will have a full example of a program that takes the PNG image displayed at @fig-pascal\nas input, and writes a new image to the current working directory that is the grayscale version of the input image.\nThis grayscale version of @fig-pascal that is written by the program is exposed at @fig-pascal-gray.\n\n![The grayscale version of the photo.](../ZigExamples/image_filter/pedro_pascal_filter.png){#fig-pascal-gray}\n\n\n## How we see things? {#sec-eyes}\n\nIn this section, I want to briefly describe to you how we (humans) actually see things with our own eyes.\nI mean, how our eyes work? If you do have a very basic understanding of how our eyes work, you will understand\nmore easily how digital images are made. Because the techniques used to produce a digital image\nwere developed by taking a lot of inspiration from how our human eyes work.\n\nYou can interpret a human eye as a light sensor, or, a light receptor. The eye receives some amount of light as input,\nand it interprets the colors that are present in this \"amount of light\".\nIf no amount of light hits the eye, then, the eye cannot extract color from it, and as result,\nwe end up seeing nothing, or, more precisely, we see complete blackness.\n\nSo everything depends on light. What we actually see are the colors (blue, red, orange, green, purple, yellow, etc.) that\nare being reflected from the light that is hitting our eyes. **Light is the source of all colors!**\nThis is what Isaac Newton discovered on his famous prism experiment[^newton] in the 1660s.\n\n[^newton]: <https://library.si.edu/exhibition/color-in-a-new-light/science>\n\nInside our eyes, we have a specific type of cell called the \"cone cell\".\nOur eye have three different types, or, three different versions of these \"cone cells\".\nEach of these three types of cone cell is very sensitive to a specific spectrum of the light,\nwhich are the spectrums that define the colors red, green and blue.\nSo, in summary, our eyes have specific types of cells that\nare highly sensitive to these three colors (red, green and blue).\n\nThese are the cells responsible for perceiving the color present in the light that hits our eyes.\nAs a result, our eyes perceives color as a mixture of these three colors (red, green and blue). By having an amount\nof each one of these three colors, and mixing them together, we can get any other visible color\nthat we want. So every color that we see is perceived as a specific mixture of blues, greens and reds,\nlike 30% of red, plus 20% of green, plus 50% of blue.\n\nWhen these cone cells perceive (or, detect) the colors that are found in the\nlight that is hitting our eyes, these cells produce electrical signals and sent them to the brain.\nOur brain interprets these electrical signals, and use them to form the image that we are seeing\ninside our head.\n\nBased on what we have discussed here, the items below describes the sequence of events that\ncomposes this very simplified version of how our human eyes work:\n\n1. Light hits our eyes.\n1. The cone cells perceive the colors that are present in this light.\n1. Cone cells produce electrical signals that describes the colors that were perceived in the light.\n1. The electrical signals are sent to the brain.\n1. Brain interprets these signals, and form the image based on the colors identified by these electrical signals.\n\n\n## How digital images work? {#sec-digital-img}\n\nA digital image is a \"digital representation\" of an image that we see with our eyes.\nIn other words, a digital image is a \"digital representation\" of the colors that we see\nand perceive through the light.\nIn the digital world, we have two types of images, which are: vector image and raster image.\nVector images are not described here. So just remember that every single aspect that we discuss\nhere in this chapter about digital images **are related solely to raster images**, and not vector images.\n\nRaster images are digital images that are represented as a 2D (two dimensional) matrix\nof pixels. In other words, every raster image is basically a rectangle of pixels. Each pixel have a particular color.\nSo, the raster image is just a 2D matrix of pixels, and each of these pixels are displayed in the screen of your computer (or the screen\nof any other device, e.g. laptop, tablet, smartphone, etc.) as colors.\n\nFor example, if the digital image have dimensions of 1920 pixels of width and 1080 pixels of height, then, the image\ncontains $1920 \\times 1080 = 2073600$ pixels in total. You could also say that the \"total area\" of the image is\nof 2073600 pixels, although the concept of \"area\" is not very used here in computer graphics.\n\nMost digital images we see in the world uses the RGB color model. RGB stands for (red, green and blue) if you did not\ndetected that yet. So the color of each pixel in these raster images are usually\nrepresented as a mixture of red, green and blue, just like in our eyes. That is, the color of each pixel is identified by a set of\nthree different integer values. Each integer value identifies the \"amount\" of each color (red, green or blue).\nFor example, the set `(199, 78, 70)` identifies a color that is close to red. We have 199 of red, 78 of green,\nand 70 of blue (RGB). In contrast, the set `(129, 77, 250)` describes a color close to purple.\n\n\n\n### Matrix of pixels\n\nOk, we know already that digital images are represented as a 2D matrix of pixels.\nBut we do not have a notion of a 2D matrix in Zig. Most low-level languages in general (Zig, C, Rust, etc.) do not have such notion.\nAs consequence, when programmers need to create a 2D matrix in low-level languages, they usually a normal 1D array\nto do so.\nYou just create an normal 1D array, and store all values from both dimensions into this 1D array.\n\nWhen we use such strategy, indexing the values from a specific dimension becomes a little harder.\nBut with some basic math we can solve this problem. As an example, let's use a very small image of 4x3 pixels,\nwhich have 12 pixels in total, that is represented by $3 \\times 12 = 36$ integer values.\nSo, we need to create an array of 36 unsigned 8-bit integer (`u8`) values to store the entire 2D matrix of pixels\nof this particular image.\n\nThe reason why unsigned 8-bit integer (`u8`) values are used to represent the amounts of each color,\ninstead of any other integer type, is because they take the minimum amount of space as possible, or,\nthe minimum amount of bits as possible. Which helps to reduces the binary size of the image, i.e. of the 2D matrix.\nAlso, they convey a good amount of precision and detail about the colors, even though they can represent\na relatively small range (from 0 to 255) of \"color amounts\".\n\nNow, suppose that we have this 1D array that represents a 2D matrix of 4x3 pixels, like the `matrix` object below.\nHow can we access the pixel at the 2nd row, and 3rd column of the matrix?\nWe just calculate the 1D index of this particular pixel by multiplying the number of the row\nby the width of the matrix (e.g. the width of the image), and adds the number of the column to it.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst matrix = []u8{\n    201, 10, 25, 185, 65, 70,\n    65, 120, 110, 65, 120, 117,\n    98, 95, 12, 213, 26, 88,\n    143, 112, 65, 97, 99, 205,\n    234, 105, 56, 43, 44, 216,\n    45, 59, 243, 211, 209, 54\n};\nconst row_num = 2;\nconst col_num = 3;\nconst index = (row_num * 4) + col_num;\ntry stdout.print(\"Pixel at 2nd row and 3rd column: {d}\\n\", .{matrix[index]});\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}